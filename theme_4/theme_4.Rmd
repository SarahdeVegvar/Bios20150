---
title: "Interactions"
author: "Stefano Allesina"
date: ""
runtime: shiny

output: 
  ioslides_presentation:
    widescreen: yes
    smaller: false
---

## The network of interactions
**It is interesting to contemplate a tangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth, and to reflect that these elaborately constructed forms, so different from each other, and dependent upon each other in so complex a manner, have all been produced by laws acting around us.**
*On the Origin of Species (1859) by Charles Darwin*

**if you look at the world in a certain way, everything is connected to everything else.**
*Foucault's Pendulum (1988) by Umberto Eco* 

## Interactions among individuals in the same population

## Types of interaction between species

- *Competition (-, -)*

- *Antagonism (+, -)*, e.g., consumption, parasitism.

- *Mutualism (+, +)*, e.g., pollination, seed-dispersal, symbiosis

- *Amensalism (-, 0)*, 

- *Commensalism (+, 0)*, 

## Network structure

## Lotka-Volterra equations

## Competition

## A model for consumption

## Cooperation

- Cooperation within cells --- is origin of eukaryotic organelles endosymbiontic?

- Cooperation between cells --- multicellularity

- Cooperation between individuals --- group hunting and defense

- Cooperation between species --- mutualism, symbiosis

## Cooperation is a selfish world $\Pi$

Cooperation has been seen as problematic: 

*"As Darwin appreciated, cooperative behaviour–—actions adapted to assist others that involve costs to the fitness of participants—poses a fundamental problem to the traditional theory of natural selection, which rests on the assumption that individuals compete to survive and breed"* (Clutton-Brock, 2009)



## Prisoner's dilemma
    C    D
-- ---- ----
C    R    S
D    T    P
-- ---- ----

- Two strategies: **C**ooperate, **D**efect
- If Player 1 (rows) plays **C** and Player 2 (cols) plays **C**, Player 1 receives **R** (reward)
- If Player 1 plays **C** and Player 2 plays **D**, Player 1 receives **S** (sucker)
- If Player 1 plays **D** and Player 2 plays **C**, Player 1 receives **T** (temptation)
- If Player 1 plays **D** and Player 2 plays **D**, Player 1 receives **P** (punishment)

$T > R > P > S$ $\;\;\;\;$ $2 R > T + S$

## A simple case
    C    D
-- ---- ----
C  1    0
D  1+k  k
-- ---- ----

- **k**: cost to cooperate
- **k < 1**

## Nash equilibrium
    C    D
-- ---- ----
C  1    0
D  1+k  k
-- ---- ----

- If Pl 2 plays **C** then Pl 1 would get **1** to cooperate, and **1 + k** to defect
- If Pl 2 plays **D** then Pl 1 would get **0** to cooperate, and **k** to defect
- It is always convenient to defect! 
- If Pl 2 is also rational, both will defect --- but then they would both receive lower payoffs than if they had cooperated!

## Iterated Prisoner's Dilemma (IPD)

- What if the game is played multiple times?
- The mathematics becomes more complex
- Many possible strategies!
- There isn't a "best" strategy: whether a strategy is advantageous or not depends on which other strategies are around (frequency dependence)

## Axelrod's tournament (1980)

Robert Axelrod, a political scientist at U Michigan, invited famed game theorists to participate in a tournament of IPD.

- Each strategy consisted of a computer program.
- Each strategy played 200 turns of IPD against other strategies in a round-robin tournament, as well as against themselves.
- Repeated 5 times to remove random fluctuations.
- 14 strategies submitted, to which Axelrod added a RND strategy.

## Some strategies
- **ALLC** Always cooperate
- **ALLD** Always defect
- **RND** Cooperate with probability 50%

## Winning strategy: Tit For Tat (TFT)

- The winner was one of the simplest strategy, consisting of only two rules:
 
1. Start by playing **C**
2. Play whatever the opponent played last time

The strategy was submitted by Anatol Rapoport, a mathematical psychologist formerly at UofC.

## Axelrod's second tournament
- The following year, the tournament was repeated: 62 entries
- The winner was again TFT! (even though everybody knew the results of the first tournament)
- In an influential book, Axelrod noted that good strategies possessed several traits:
- Be nice (don't be the first to defect)
- Be provocable (retaliate if other player does not cooperate)
- Don't be envious (care about your score, not that of the opponent)
- Don't be clever

## Many more tournaments!
- The tournaments are still played today
- Some strategies are extremely complex
- E.g. start playing a certain sequence to see whether two programs are on the same "team", if so, take master/slave roles

## TFT's problem: unforgiving
The main problem of TFT is that it is unforgiving: once the opponent retaliates, it triggers a cascade of retaliations. 
This is problematic when communication is not perfect (either you play **D** by mistake, or a **C** is mistaken for a **D**). For example, two TFT playing in a noisy environment:

**TFT1** CCCCCCCC**D**DDDDDDDDDDDDD

**TFT2** CCCCCCCCCDDDDDDDDDDDDD

## Generous TFT (GTFT)
Generous Tit For Tat tries to escape this cascade of retaliation by being "forgiving": it will try restoring cooperation by playing **C** with a certain probability. 

- Start with **C**
- If the opponent plays **C**, respond with **C**
- If the opponent plays **D**, respond with **C** with probability 1-k; otherwise play **D**
- The probability depends on the cost of cooperating

## Other simple strategies
**WSLS** (Win Stay, Lose Shift)

- If the previous move was successful, keep playing it; otherwise, switch to the opposite move.

**GRIM** (Grim Trigger)

- If the previous move both player played **C**, cooperate; otherwise defect

## Classification of strategies
- Deterministic vs Stochastic: does the strategy involve randomness?
- Reactive vs Non-Reactive: does it react to previous moves?
- Memory 0, Memory 1/2, Memory 1: the strategy uses no information on the previous move (Memory 0); information on the previous move of the opponent (Memory 1/2); information on the previous move of both player (Memory 1).

Examples:
- **RND** (Stochastic, Non-Reactive, Memory 0)
- **TFT** (Deterministic, Reactive, Memory 1/2)
- **GTFT** (Stochastic, Reactive, Memory 1/2)
- **GRIM** (Deterministic, Reactive, Memory 1)

## Supergames

- For the IPD, it is important that the players do not know how many tournaments will be played
- Otherwise, it is convenient to defect at the last round, but then both player will defect at the last round, making it convenient to defect at the penultimate round, and so on.
- One mathematically convenient approximation is that in which infinitely many rounds are played---early moves do not matter
- The task of modeling infinite games (supergames) is easier if each player has a small probability of getting confused, playing the "wrong" move

## Supergames: math $\Pi$

Vector $\mathbf{p}$ encodes the probability of playing $C$ given the previous move of both players.

$\mathbf{p} = \left\{ p_{CC}, p_{CD}, p_{DC}, p_{DD} \right\}$

e.g.:

- RND $\mathbf{p} = \left\{0.5,0.5,0.5,0.5\right\}$

- ALLD $\mathbf{p} = \left\{0,0,0,0\right\}$

- ALLC $\mathbf{p} = \left\{1,1,1,1\right\}$

## Supergames: math

- TFT $\mathbf{p} = \left\{1,0,1,0\right\}$

- GTFT $\mathbf{p} = \left\{1,1-k,1,1-k\right\}$

- GRIM $\mathbf{p} = \left\{1,0,0,0\right\}$

- WSLS $\mathbf{p} = \left\{1,0,0,1\right\}$

## Supergames: math
Probability of making mistakes: $\epsilon$

$\mathbf{p'} = (1-\epsilon) \mathbf{p} + \epsilon(\mathbf{1} - \mathbf{p})$

e.g. 

- TFT $\mathbf{p'} = \left\{1 - \epsilon, \epsilon, 1 - \epsilon, \epsilon\right\}$
- GRIM $\mathbf{p'} = \left\{1 - \epsilon, \epsilon, \epsilon, \epsilon\right\}$

## Supergames: math
- We can model the evolution of the game as a Markov Chain with four states, denoting the moves played by the two players at time $t$: CC, CD, DC, DD.
- Player 1 plays $\mathbf{p'}$; Player 2 plays $\mathbf{q'}$

$\mathbf M = \left(\begin{array}[cccc]
{}\mathbf{p'}_{1} \mathbf{q'}_{1} & \mathbf{p'}_{1} (1- \mathbf{q'}_{1}) & (1- \mathbf{p'}_{1}) \mathbf{q'}_{1} & (1 - \mathbf{p'}_{1}) (1- \mathbf{q'}_{1})\\
\mathbf{p'}_{2} \mathbf{q'}_{3} & \mathbf{p'}_{2} (1- \mathbf{q'}_{3}) & (1- \mathbf{p'}_{2}) \mathbf{q'}_{3} & (1 - \mathbf{p'}_{2}) (1- \mathbf{q'}_{3})\\
\mathbf{p'}_{3} \mathbf{q'}_{2} & \mathbf{p'}_{3} (1- \mathbf{q'}_{2}) & (1- \mathbf{p'}_{3}) \mathbf{q'}_{2} & (1 - \mathbf{p'}_{3}) (1- \mathbf{q'}_{2})\\
\mathbf{p'}_{4} \mathbf{q'}_{4} & \mathbf{p'}_{4} (1- \mathbf{q'}_{4}) & (1- \mathbf{p'}_{4}) \mathbf{q'}_{4} & (1 - \mathbf{p'}_{4}) (1- \mathbf{q'}_{4})
\end{array}
\right)$

- Note that row sum is 1 for all rows

## Stationary distribution

- Because this is a Markov Chain, and because we have the small $\epsilon$ guaranteeing that all coefficients are nonzero, the game will eventually converge to a stationary distribution.

- We can project the game forward $S_t =$  probability of being in each state at time $t$

- $S_{t + 1} = S_t M$

- We rapidly approach a distribution of probabilities that does not change through time:

- $v M = v$

## Stationary distribution
```{r echo = FALSE, warning=FALSE, message=FALSE}
fluidRow(
  column(3,
  selectInput(inputId = "pl1",
                label = "Player 1:",
                choices = c(
                  "ALLC" = "ALLC",
                  "ALLD" = "ALLD",
                  "RND"  = "RND",
                  "TFT"  = "TFT",
                  "GTFT"  = "GTFT",
                  "GRIM" = "GRIM",
                  "WSLS" = "WSLS"),
                selected = "WSLS")),
  column(3,
  selectInput(inputId = "pl2",
                label = "Player 2:",
                choices = c(
                  "ALLC" = "ALLC",
                  "ALLD" = "ALLD",
                  "RND"  = "RND",
                  "TFT"  = "TFT",
                  "GTFT"  = "GTFT",
                  "GRIM" = "GRIM",
                  "WSLS" = "WSLS"),
                selected = "GTFT")),
  column(3,
    numericInput('ep', 'prob. error', 0.01,
                 min = 0, max = 1.0, step = 0.01)),
    
    column(3,
           numericInput('k', 'Cost cooperation', 0.5,
                 min = 0, max = 1, step = 0.1))
    )
```
```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
source("code/ShowMC.R")
renderPlot(height = 400, width = 800, {
    show(ShowSimulation(input$pl1, input$pl2, input$ep, input$k))
})
```

## Evolutionary game theory
- We have a population of individuals
- Each individual has genes encoding a strategy
- Mutations lead to individuals with different strategies
- Mutations are rare: in this limit we will have at most two strategies at any time
- Can mutations spread in the population?
- Only if "mutant" can invade "wildtype"
- We need to consider the average payoff (fitness) of the mutant against wildtype, mutant against mutant, wildtype against wildtype

## Calculating average payoff (fitness)

- Matrix $M$
- Stationary distribution $\mathbf{v}$
- Player 1 plays $\mathbf{p'}$, Player 2 plays $\mathbf{q'}$
- Average payoff of Player 1: $\pi (\mathbf{p'}, \mathbf{q'}) = \mathbf{v} \mathbf{h_1}$, where $\mathbf{h_1} = \left\{ 1, 0, 1 + k, k\right\}$
- Average payoff of Player 2: $\pi (\mathbf{q'}, \mathbf{p'}) = \mathbf{v} \mathbf{h_2}$, where $\mathbf{h_2} = \left\{ 1, 1 + k, 0, k\right\}$

## Avergage Fitness

```{r echo = FALSE, warning=FALSE, message=FALSE}
fluidRow(
  column(3,
  selectInput(inputId = "pl1_2",
                label = "Player 1:",
                choices = c(
                  "ALLC" = "ALLC",
                  "ALLD" = "ALLD",
                  "RND"  = "RND",
                  "TFT"  = "TFT",
                  "GTFT"  = "GTFT",
                  "GRIM" = "GRIM",
                  "WSLS" = "WSLS"),
                selected = "ALLC")),
  column(3,
  selectInput(inputId = "pl2_2",
                label = "Player 2:",
                choices = c(
                  "ALLC" = "ALLC",
                  "ALLD" = "ALLD",
                  "RND"  = "RND",
                  "TFT"  = "TFT",
                  "GTFT"  = "GTFT",
                  "GRIM" = "GRIM",
                  "WSLS" = "WSLS"),
                selected = "ALLD")),
  column(3,
    numericInput('ep_2', 'prob. error', 0.01,
                 min = 0, max = 1.0, step = 0.01)),
    
    column(3,
           numericInput('k_2', 'Cost cooperation', 0.7,
                 min = 0, max = 1, step = 0.1))
    )
```
```{r, echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
source("code/MCIPD.R")
renderPlot(height = 400, width = 800, {
    show(PlotPayOffs(input$pl1_2, input$pl2_2, input$ep, input$k_2))
})
```

## War and Peace

## Other mechanisms for cooperation

- We have played with **direct reciprocity** (I scratch your back, you scratch my back)
- Another key mechanism is **indirect reciprocity** (golden rule --- I scratch your back, somebody will scratch mine)
- Indirect reciprocity can lead to emergence of cooperation when **reputation** is at stake

## Spatial cooperation $\Pi$

- Cooperation can also emerge in a spatial context -- **ALLD** cannot wipe out **ALLC** in a spatial game

<div class="centered">
<img src="./images/spatial1.jpg" alt="spatial" height="400"> 

Nowak and May, 1992
</div>

## Spatial cooperation $\Pi$

<div class="centered">
<img src="./images/spatial2.jpg" alt="spatial" height="520"> 

Nowak and May, 1992
</div>

## Multilevel selection

- Evolutionary game theory has been applied to multilevel selection
- Group selection
- Kin selection

## Cooperation wrapup

